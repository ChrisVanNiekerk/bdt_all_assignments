{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages and start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd \n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, Imputer, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "spark = SparkSession.builder.appName('random_forest_spark').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get External dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307511, 4)\n"
     ]
    }
   ],
   "source": [
    "url = \"https://storage.googleapis.com/bdt-spark-store/external_sources.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df_ext = spark.read.csv(\"file:///\"+SparkFiles.get(\"external_sources.csv\"), header=True, inferSchema= True)\n",
    "print((df_ext.count(), len(df_ext.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307511, 119)\n"
     ]
    }
   ],
   "source": [
    "url = \"https://storage.googleapis.com/bdt-spark-store/internal_data.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df_data  = spark.read.csv(\"file:///\"+SparkFiles.get(\"internal_data.csv\"), header=True, inferSchema= True)\n",
    "print((df_data.count(), len(df_data.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307511, 122)\n"
     ]
    }
   ],
   "source": [
    "df_full = df_data.join(df_ext, on='SK_ID_CURR', how='inner')\n",
    "print((df_full.count(), len(df_full.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter for columns required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_extract = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\n",
    "                  'DAYS_BIRTH', 'DAYS_EMPLOYED', 'NAME_EDUCATION_TYPE',\n",
    "                  'DAYS_ID_PUBLISH', 'CODE_GENDER', 'AMT_ANNUITY',\n",
    "                  'DAYS_REGISTRATION', 'AMT_GOODS_PRICE', 'AMT_CREDIT',\n",
    "                  'ORGANIZATION_TYPE', 'DAYS_LAST_PHONE_CHANGE',\n",
    "                  'NAME_INCOME_TYPE', 'AMT_INCOME_TOTAL', 'OWN_CAR_AGE', 'TARGET']\n",
    "df = df_full[columns_extract]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_variables = ['NAME_EDUCATION_TYPE', 'CODE_GENDER', 'ORGANIZATION_TYPE', 'NAME_INCOME_TYPE']\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"-index\") for column in categorical_variables]\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[indexer.getOutputCol() for indexer in indexers],\n",
    "    outputCols=[\"{0}-encoded\".format(indexer.getOutputCol()) for indexer in indexers]\n",
    ")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=encoder.getOutputCols(),\n",
    "    outputCol=\"categorical-features\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + [encoder, assembler])\n",
    "df_oh = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (246051, 27)\n",
      "Testing data shape:  (61460, 27)\n"
     ]
    }
   ],
   "source": [
    "train, test = df_oh.randomSplit([0.8, 0.2], seed=101)\n",
    "print('Training data shape: ', (train.count(), len(train.columns)))\n",
    "print('Testing data shape: ', (test.count(), len(test.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate target variable proportions in each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------------------+\n",
      "|TARGET| count|   train_proportion|\n",
      "+------+------+-------------------+\n",
      "|     1| 19861|0.08071903792303221|\n",
      "|     0|226190| 0.9192809620769677|\n",
      "+------+------+-------------------+\n",
      "\n",
      "None\n",
      "+------+-----+-------------------+\n",
      "|TARGET|count|    test_proportion|\n",
      "+------+-----+-------------------+\n",
      "|     1| 4964|0.08076797917344615|\n",
      "|     0|56496| 0.9192320208265539|\n",
      "+------+-----+-------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train_prop = train.groupBy('TARGET').count()\n",
    "test_prop = test.groupBy('TARGET').count()\n",
    "print(train_prop.withColumn('train_proportion', train_prop['count'] / train.count()).show())\n",
    "print(test_prop.withColumn('test_proportion', test_prop['count'] / test.count()).show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer()\n",
    "\n",
    "imputer.setInputCols(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'DAYS_LAST_PHONE_CHANGE', 'OWN_CAR_AGE'])\n",
    "\n",
    "imputer.setOutputCols(['out_EXT_SOURCE_1', 'out_EXT_SOURCE_2', 'out_EXT_SOURCE_3', 'out_AMT_ANNUITY', 'out_AMT_GOODS_PRICE', 'out_DAYS_LAST_PHONE_CHANGE', 'out_OWN_CAR_AGE'])\n",
    "\n",
    "imputer.getRelativeError()\n",
    "\n",
    "imputer.setStrategy('median')\n",
    "\n",
    "model = imputer.fit(train)\n",
    "model.setInputCols(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'DAYS_LAST_PHONE_CHANGE', 'OWN_CAR_AGE'])\n",
    "train_filled = model.transform(train)\n",
    "\n",
    "model = imputer.fit(test)\n",
    "model.setInputCols(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'DAYS_LAST_PHONE_CHANGE', 'OWN_CAR_AGE'])\n",
    "test_filled = model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop redundant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'DAYS_LAST_PHONE_CHANGE', 'OWN_CAR_AGE']\n",
    "train_filled_dropped = train_filled.drop(*columns_to_drop)\n",
    "test_filled_dropped = test_filled.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_variables = ['DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_ID_PUBLISH', 'DAYS_REGISTRATION',\n",
    "                        'AMT_CREDIT', 'AMT_INCOME_TOTAL', 'TARGET', 'out_AMT_ANNUITY',\n",
    "                        'out_DAYS_LAST_PHONE_CHANGE', 'out_EXT_SOURCE_1', 'out_OWN_CAR_AGE',\n",
    "                        'out_EXT_SOURCE_2', 'out_AMT_GOODS_PRICE', 'out_EXT_SOURCE_3']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[*continuous_variables], outputCol='continuous_features')\n",
    "\n",
    "train_filled_dropped = assembler.transform(train_filled_dropped)\n",
    "test_filled_dropped = assembler.transform(test_filled_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol='continuous_features', outputCol='scaled_continuous_features')\n",
    "\n",
    "scaler_model = scaler.fit(train_filled_dropped)\n",
    "train_final_scaled = scaler_model.transform(train_filled_dropped)\n",
    "\n",
    "scaler_model = scaler.fit(test_filled_dropped)\n",
    "test_final_scaled = scaler_model.transform(test_filled_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine categorical vector with continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['categorical-features', 'continuous_features'], outputCol='features')\n",
    "\n",
    "var_names = assembler\n",
    "train_final = assembler.transform(train_final_scaled)\n",
    "test_final = assembler.transform(test_final_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"TARGET\", featuresCol=\"features\", numTrees=100, seed=50, impurity='gini')\n",
    "\n",
    "model = rf.fit(train_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.963651\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = ulticlassClassificationEvaluator(labelCol=\"TARGET\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "# acc = evaluator.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"accuracy\"})\n",
    "print(\"Accuracy = %g\" % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84,[0,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16,18,19,20,21,22,25,27,28,30,32,33,34,35,37,38,40,41,42,45,49,52,53,57,58,59,62,63,64,65,66,67,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83],[0.0014837734904038855,0.003965600742873514,6.6428243489286556e-06,5.169548769134106e-06,0.00541254103793424,0.0015421031043257598,2.2697431107516686e-05,0.0006918241034137973,5.717870628748703e-05,1.0317759049543155e-05,0.0002515782990983191,1.9603061927268928e-05,9.73355533651983e-05,4.315486706163916e-06,7.1796237686768594e-06,2.486547883743997e-05,1.5728814025436644e-06,4.750877017655322e-05,3.888380461296613e-05,5.679880321673615e-06,2.814413002235665e-05,7.090293593677337e-06,1.842839977285682e-05,6.447604116689151e-06,9.823526484480058e-06,6.625462502996261e-06,8.237518074385378e-06,2.4547770570924784e-05,1.026811354010322e-05,4.25454849148211e-05,1.2472352066861696e-05,1.379770475713538e-05,3.111562013481721e-05,3.736865482333044e-06,4.758518560535429e-06,2.4816574512289112e-05,2.4058726655644958e-06,1.8816847428369692e-06,5.178125194678104e-06,1.9669184175785243e-06,1.4948371532818844e-06,3.5316740292320554e-06,0.00490637134873488,8.618672673358429e-05,0.001459496139498496,2.1557704557627444e-05,1.8371861870371865e-05,2.4470867127965544e-08,0.003854115483355744,0.0015037893211886201,0.0002461141079598304,0.0005198072403031181,0.0002885121899682467,0.00014647412446252997,0.9181795236572726,0.0009872456134491557,0.000501970842826258,0.008165514107503711,0.0007084123070991428,0.013609075190244213,0.002675335919768582,0.02815641500229517])\n"
     ]
    }
   ],
   "source": [
    "print(model.featureImportances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
